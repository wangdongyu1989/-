
## Memory-Management

可想而知，内核在运行中常常需要使用一些缓冲区。例如，当要建立一个新的进程时就要增加一个task_struct结构，而当进程撤销时就要释放本进程的task结构。这些小块存储空间的使用并不局限某一个子进程，否则就可以作为这个子程序的局部变量而使用堆栈空间了。另外，这些小块存储空间又是动态变化的，不像用于页面管理的page结构那样，有多大的内存就有多少个page结构，构成一个静态的阵列。由于事先根本分无法预测运行中各种不同数据结构对缓存区的需求，不适合为每一种可能用到的数据结构建立一个“缓冲池”，因为那样的话很可能会出现有些缓存池已经用尽而有些缓冲池中却有大量缓冲区的局面。因此，只能采用更具有全局性的方法。

那么，用什么样的方法呢，如果采用像用户空间的malloc那样的动态分配办法，从一个统一的存储空间“堆”（heap）中，需要多少就切多大一块，不用就归还，则有几个缺点需要考虑改进：

* 久而久之，会使存储堆“碎片化”，以至虽然存储堆中空闲空间的总量足够大，却无法分配所需大小的连续空间。为此，一般都采用按2的次幂的大小分配空间，以缓解碎片化。

* 每次分配得到所需大小的缓冲区以后，都要进行初始化。内核中频繁地使用一些数据结构，这些数据结构中相当一部分成分需要某些特殊的初始化（例如队列头部等）而并非简单地清成0。如果释放的数据结构可以在下次分配时“重用”而无需初始化，那就可以提高内核的效率。

* 缓冲区的组织和管理是密切相关。在有高速缓冲区的情况下，这些缓冲区的组织和管理方式直接影响到高速缓存中的命中率，进而影响到运行时的效率。试想，假定我们运用最先符合(first fit)的方法，从一个由存储空间片段构成的队列中分配缓冲区。在这样的过程中，当一个片段不能满足要求而顺着指针往下看下一个片段的数据结构时，如果该数据结构每次都在不同的片段中，因而每次都不能命中，而要从内存转入到高速缓存，那么可想而知，其效率显然要打折扣了。

* 不适合多处理器共用内存的情况。

在slab方法中，每种重要的数据结构都有自己专用的缓冲区队列，每种数据结构都有相应的“构造”和“拆除”函数。同时，还借用面向对象程序设计中的名词，不再称“结构”而称为“对象”。缓冲区队列中的各个对象在建立时用其“构造”函数进行初始化，所以一经分配立即就能使用，而在释放时则恢复成原状。例如，对于其中的队列头成分来说，当将其从队列中摘除时自然就恢复成了原状。每个队列中“对象”的个数是动态变化的，不够时可以增添。同时，又定期地检查，将有富余的队列加以精简。

此外，slab管理方法还有一个特点，每种对象的缓冲区队列并非由各个对象直接构成，而是由一连串“大块（slab）”构成,而每个大块中则包含了若干同种的对象。一般而言，对象分两种，一种是大对象，一种是小对象。所谓小对象，是指在一个页面中可以包容下好几个对象的那一种。例如，一个inode的大小约300多个字节，因此一个页面可以容纳8个以上的inode，所以inode是小对象。内核中使用的大多数数据结构都是这样的小对象，所以，我们先来看小对象的组织和管理以及相应的slab结构。先看用于某种假象小对象的一个slab块的结构示意图。

![image](https://github.com/wangdongyu1989/Memory-Management/blob/master/images/%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%8420170808a.jpg)

此处先对上述示意图作几点说明，详细情况则随着代码的阅读再逐步深入：

*  一个slab可能由1个，2个，4个,...最多32个连续的物理页面构成。slab的具体大小因对象的大小而异，初始化时通过计算得出最合适的大小。

*  在每个slab的前端是该slab的描述符结构slab_t。用于同一种对象的多个slab通过描述符中的队列头形成一条双相链队列。每个slab双向链队列在逻辑上分成三截，第一截是各个slab上所有对象都已分配使用；第二截是各个slab上对象已经部分地分配使用；最后一截是各个slab上全部对象都处于空闲状态。

*  每个slab都有一个对象区，这个是对象数据结构的数组，以对象的序号为下标就可得到具体对象的起始地址。

*  每个slab上还有个对象链接数组，用来实现一个空闲对象链。

*  同时，每个slab的描述结构中都有一个字段，表明该slab上的第一个空闲对象。这个字段与对象链接数组结合在一起形成了一条空闲对象链。

*  在slab描述结构中还有一个已经分配使用的对象的计数器，当将一个空闲的对象分配使用时，就将slab控制结构中的计数器加1，并将改对象从空闲列表中脱链。

*  当释放一个对象时，只需要调整链接数组中相应元素以及slab描述结构中的计数器，并且根据该slab的使用情况而调整其在slab队列中的位置。

*  每个slab的头部有一个小小的区域是不使用的，称为“着色区”。着色区的大小的大小使slab中的每个对象的起始地址都按高速缓存中的“缓冲行”大小对齐。每个slab都是从一个页面边界开始的，所以本来就自然按高速缓存的缓冲行对齐，而着色区的设置只是将第一个对象的起始地址往后推导另一个与缓冲行对齐的边界。同一个对象的缓冲队列中的各个slab的着色区的大小尽可能安排成不同的大小，使得不同slab上同一相对位置的对象的起始地址在高速缓存中相互错开，这个可以改善高速缓存的效率。

*  每个对象的大小基本上是所需数据结构的大小。只有当数据结构的大小不与高速缓存中缓冲对齐时，才增加若干字节使其对齐。所以，一个slab上的所有对象的起始地址都必然是按高速缓存中的缓冲行对齐的。

下面就是slab描述结构的slab_t的定义，在mm/slab.c中：

```C++
==================== mm/slab.c 138 152 ====================
138  /*
139   * slab_t
140   *
141   * Manages the objs in a slab. Placed either at the beginning of mem allocated
142   * for a slab, or allocated from an general cache.
143   * Slabs are chained into one ordered list: fully used, partial, then fully
144   * free slabs.
145   */
146  typedef struct slab_s {
147     struct list_head  list;
148     unsigned long colouroff;
149     void *s_mem; /* including colour offset */
150     unsigned int inuse; /* num of objs active in slab */
151     kmem_bufctl_t free;
152  } slab_t;
```

这里的队列头list用来将一块slab链入一个专用缓冲区队列，colouroff为本slab上着色区的大小，指针s_mem指向对象区的起点，inuse是已分配对象的计数器。最后，free的值表明了空闲对象链中的第一个对象，其实是个整数：

```c++
==================== mm/slab.c 110 131 ====================
110  /*
111   * kmem_bufctl_t:
112   *
113   * Bufctl's are used for linking objs within a slab
114   * linked offsets.
115   *
116   * This implementaion relies on "struct page" for locating the cache &
117   * slab an object belongs to.
118   * This allows the bufctl structure to be small (one int), but limits
119   * the number of objects a slab (not a cache) can contain when off-slab
120   * bufctls are used. The limit is the size of the largest general cache
121   * that does not use off-slab slabs.
122   * For 32bit archs with 4 kB pages, is this 56.
123   * This is not serious, as it is only for large objects, when it is unwise
124   * to have too many per slab.
125   * Note: This limit can be raised by introducing a general cache whose size
126   * is less than 512 (PAGE_SIZE<<3), but greater than 256.
127   */
128
129  #define BUFCTL_END 0xffffFFFF
130  #define  SLAB_LIMIT 0xffffFFFE
131  typedef unsigned int kmem_bufctl_t;
```

在空闲对象链接数组中，链内每一个对象所对应元素的值为下一个对象序号，最后一个对象对对应的值为BUFCTL_END。

为每种对象建立的slab队列都有个队列头，其控制结构为kmem_cache_t。该数据结构中除用来维持slab队列的各种指针外，还记录了使用于队列中每个slab的各种参数
，以及两个函数指针：一个是对象的构造函数，另一个是拆除函数。有趣的是，像其他数据结构一样，每种对象的slab队列头也是在slab上。系统中有个总的slab队列，其对象是各个其他对象的slab队列头，其队列头则是一个kmem_cache_t结构，称为cache_cache。

这样，就形成一种层次式的树形结构：

*  总跟cache_cache是一个kmem_cache_t结构，用来维持第一层slab队列，这些slab队列的对象都是kmem_cache_t数据结构。

*  每个第一层slab上的每个对象，即kmem_cache_t数据结构都是队列头，用来维持一个第二层slab队列。

*  第二层slab队列基本上都是为某种对象，即数据结构专用的。

*  每个第二层slab上都维持着一个空闲对象队列。

*  同时，每个slab的描述结构中都有一个字段，表明该slab上的第一个空闲对象。这个字段与对象链接数组结合在一起形成了一条空闲对象链。

*  在slab描述结构中还有一个已经分配使用的对象的计数器，当将一个空闲的对象分配使用时，就将slab控制结构中的计数器加1，并将改对象从空闲列表中脱链。

*  当释放一个对象时，只需要调整链接数组中相应元素以及slab描述结构中的计数器，并且根据该slab的使用情况而调整其在slab队列中的位置。

*  每个slab的头部有一个小小的区域是不使用的，称为“着色区”。着色区的大小的大小使slab中的每个对象的起始地址都按高速缓存中的“缓冲行”大小对齐。每个slab都是从一个页面边界开始的，所以本来就自然按高速缓存的缓冲行对齐，而着色区的设置只是将第一个对象的起始地址往后推导另一个与缓冲行对齐的边界。同一个对象的缓冲队列中的各个slab的着色区的大小尽可能安排成不同的大小，使得不同slab上同一相对位置的对象的起始地址在高速缓存中相互错开，这个可以改善高速缓存的效率。

*  每个对象的大小基本上是所需数据结构的大小。只有当数据结构的大小不与高速缓存中缓冲对齐时，才增加若干字节使其对齐。所以，一个slab上的所有对象的起始地址都必然是按高速缓存中的缓冲行对齐的。

下面就是slab描述结构的slab_t的定义，在mm/slab.c中：

```C++
==================== mm/slab.c 138 152 ====================
138  /*
139   * slab_t
140   *
141   * Manages the objs in a slab. Placed either at the beginning of mem allocated
142   * for a slab, or allocated from an general cache.
143   * Slabs are chained into one ordered list: fully used, partial, then fully
144   * free slabs.
145   */
146  typedef struct slab_s {
147     struct list_head  list;
148     unsigned long colouroff;
149     void *s_mem; /* including colour offset */
150     unsigned int inuse; /* num of objs active in slab */
151     kmem_bufctl_t free;
152  } slab_t;
```

这里的队列头list用来将一块slab链入一个专用缓冲区队列，colouroff为本slab上着色区的大小，指针s_mem指向对象区的起点，inuse是已分配对象的计数器。最后，free的值表明了空闲对象链中的第一个对象，其实是个整数：

```c++
==================== mm/slab.c 110 131 ====================
110  /*
111   * kmem_bufctl_t:
112   *
113   * Bufctl's are used for linking objs within a slab
114   * linked offsets.
115   *
116   * This implementaion relies on "struct page" for locating the cache &
117   * slab an object belongs to.
118   * This allows the bufctl structure to be small (one int), but limits
119   * the number of objects a slab (not a cache) can contain when off-slab
120   * bufctls are used. The limit is the size of the largest general cache
121   * that does not use off-slab slabs.
122   * For 32bit archs with 4 kB pages, is this 56.
123   * This is not serious, as it is only for large objects, when it is unwise
124   * to have too many per slab.
125   * Note: This limit can be raised by introducing a general cache whose size
126   * is less than 512 (PAGE_SIZE<<3), but greater than 256.
127   */
128
129  #define BUFCTL_END 0xffffFFFF
130  #define  SLAB_LIMIT 0xffffFFFE
131  typedef unsigned int kmem_bufctl_t;
```

在空闲对象链接数组中，链内每一个对象所对应元素的值为下一个对象序号，最后一个对象对对应的值为BUFCTL_END。

为每种对象建立的slab队列都有个队列头，其控制结构为kmem_cache_t。该数据结构中除用来维持slab队列的各种指针外，还记录了使用于队列中每个slab的各种参数
，以及两个函数指针：一个是对象的构造函数，另一个是拆除函数。有趣的是，像其他数据结构一样，每种对象的slab队列头也是在slab上。系统中有个总的slab队列，其对象是各个其他对象的slab队列头，其队列头则是一个kmem_cache_t结构，称为cache_cache。

这样，就形成一种层次式的树形结构：

*  总跟cache_cache是一个kmem_cache_t结构，用来维持第一层slab队列，这些slab队列的对象都是kmem_cache_t数据结构。

*  每个第一层slab上的每个对象，即kmem_cache_t数据结构都是队列头，用来维持一个第二层slab队列。

*  第二层slab队列基本上都是为某种对象，即数据结构专用的。

*  每个第二层slab上都维持着一个空闲对象队列。

![image](https://github.com/wangdongyu1989/Memory-Management/blob/master/images/%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%8420170809a.jpg)

如图，最高层次式slab队列cache_cache，队列中的每个slab载有若干个kmem_cache_t数据结构。而每个这样的数据结构又是某种数据结构（例如inode，vm_area_struct,mm_struct,乃至于IP网络信息包等等）缓冲区的slab队列的头部。这样，当要分配一个某种数据结构的缓冲区时，就只要指明是从哪一个队列中分配，而不需要说明缓冲区的大小，并且不需要初始化了。

当数据结构比较大，因而不属于“小对象”时，slab的结构略有不同。不同之处是不将slab的控制结构放在他所代表的slab上，而是将其游离出来，集中放到另外的slab上。由于在slab的控制结构kmem_slab_t中有一个指针指向相应slab上的第一个对象，所以逻辑上是一样的。其实，这就是将控制结构与控制对象相分离的一般模式。此外，当对象的大小恰好是物理页面的1/2,1/4或1/8时，将依附于每个对象的链接指针紧挨着放在一起会造成slab空间上的重大浪费，所以在这些情况下，将链接指针也从
slab上游离出来集中存放，以提高slab的空间使用率。


不过，并非内核中使用的所有数据结构都有必要拥有专用的缓冲区队列，一些不太常用，初始化开销也不大的数据结构还是可以何用一个通用的缓冲区分配机制。所以，Linux内核中还有一种既类似于物理页面分配中采用的按大小分区，又采用slab方式管理的通用缓冲池，称为"slab_cache"。slab_cache的结构与cache_cache大同小异，只不过其顶层不是一个队列而是一个结构数组，数组中的每一个元素执行一个不同的slab队列。这些slab队列的不同之处在于所载对象的大小。最小的是32，然后依次是64,128...直至128K。从通用来缓冲池分配和释放缓冲区的函数为：

```c++
   void *kmalloc(size_t size, int flags);
   void kfree(const void *objp);
```
所以，当需要分配一个不具有专用slab队列的数据结构而又不必为之使用整个页面时，就应该通过kmalloc分配。如果数据结构的大小接近一个页面，则也可以干脆通过alloc_pages（）为之分配一个页面。

本来，虚存区间结构vm_area_struct的专用缓冲队列是一个很好的实例，读者都已经熟悉了这个数据结构的使用。但是，到现在为止，Linux内核中多数专用缓冲区的建立都使用NULL作为构造函数的指针，也就是说没有充分利用slab管理机制所提供的好处，似乎不够典型。所以，我们选择从内核的网络驱动子系统中选择了一个例子，这是在net/core/skbuff.c中定义的：

```c++
==================== net/core/skbuff.c 473 487 ====================
473  void __init skb_init(void)
474  {
475 int i;
476
477 skbuff_head_cache = kmem_cache_create("skbuff_head_cache",
478       sizeof(struct sk_buff),
479       0,
480       SLAB_HWCACHE_ALIGN,
481       skb_headerinit, NULL);
482       if (!skbuff_head_cache)
483       panic("cannot create skbuff cache");
484
485       for (i=0; i<NR_CPUS; i++)
486              skb_queue_head_init(&skb_head_pool[i].list);
487  }
```

从代码中可以看到，skb_init所做的事情实际上就是为网络驱动子系统建立一个sk_buff数据结构的专用缓冲区队列，其名称为“skbuf_head_cache”。读者可用命令“cat /pro/slbinfo”来观察这些队列的使用情况。每个缓冲区，或者说“对象”的大小是sizeof(struct sk_buff)。调用参数offset为0，表示对第一个缓冲区在slab中的位移并无特殊要求。但是参数flags为SLAB_HWCHCHE_ALIGN,表示要求与高速缓存中的缓冲行边界对齐。对象的构造函数为skb_headerinit(),而destructor则为NULL，也就是说拆除或释放一个slab时无需对各个缓冲区进行特殊的处理。

首先，要从cache_cache中分配一个kmem_cache_t结构，作为sk_buff数据结构slab队列的控制结构。数据结构类型kmem_cache_t是在mm/slab.c中定义的：


```C++
==================== mm/slab.c 181 237 ====================
181  struct kmem_cache_s {
182  /* 1) each alloc & free */
183 /* full, partial first, then free */
184   struct list_head  slabs;
185   struct list_head  *firstnotfull;
186   unsigned int objsize;
187   unsigned int flags;  /* constant flags */
188   unsigned int num; /* # of objs per slab */
189   spinlock_t spinlock;
190   #ifdef CONFIG_SMP
191      unsigned int batchcount;
192   #endif
193
194  /* 2) slab additions /removals */
195  /* order of pgs per slab (2^n) */
196   unsigned int gfporder;
197
198  /* force GFP flags, e.g. GFP_DMA */
199   unsigned int gfpflags;
200
201   size_t colour; /* cache colouring range */
202   unsigned int colour_off;  /* colour offset */
203   unsigned int colour_next;  /* cache colouring */
204   kmem_cache_t *slabp_cache;
205   unsigned int growing;
206   unsigned int dflags; /* dynamic flags */
207
208 /* constructor func */
209   void (*ctor)(void *, kmem_cache_t *, unsigned long);
210
211 /* de-constructor func */
212   void (*dtor)(void *, kmem_cache_t *, unsigned long);
213
214   unsigned long failures;
215
216  /* 3) cache creation/removal */
217   char name[CACHE_NAMELEN];
218   struct list_head  next;
219  #ifdef CONFIG_SMP
220  /* 4) per-cpu data */
221   cpucache_t *cpudata[NR_CPUS];
222  #endif
223  #if STATS
224   unsigned long num_active;
225   unsigned long num_allocations;
226   unsigned long high_mark;
227   unsigned long grown;
228   unsigned long reaped;
229   unsigned long errors;
230  #ifdef CONFIG_SMP
231   atomic_t allochit;
232   atomic_t allocmiss;
233   atomic_t freehit;
234   atomic_t freemiss;
235  #endif
236  #endif
237  };

12  typedef struct kmem_cache_s kmem_cache_t;

```

结构中队列头slabs用来维持一个slab队列，指针firstnotfull则指向队列中第一个含有空闲对象的slab，也就是指向队列中的第2段。当这个指针指向队列头slabs时就表明队列中不存在还有空闲对象的slab。


结构中还有队列头next，则用来在cache_cache中建立一个“专用缓冲区slab队列的队列”，也就是slab队列控制结构的队列。当slab的描述结构与对象不在同一个slab上时，即对于大对象slab,针对slabp_cache指向对方队列的控制结构。


除了这些队列头和指针以外，还有一些重要的成分：object是原始的数据结构（对象）的大小，在这个情景中就是sizeof(struct sk_buff); num表示每个slab上有几个缓冲区；gfporder则表示每个slab的大小，每个slab都是由2的n次幂页面构成的，而gfporder为n。

前面讲过，在每个slab的前部分保留一个小块区域空着不用，那就是“着色区”，其作用是使用同一slab队列中不同slab上对象的起始地址错开，这样有利于改善高速缓冲的效率。所以，如果当前slab的颜色为1，则下一个slab的颜色将是2，使下一个slab中的第一个缓冲区更往后推一些。但是，不同“颜色”的数量是有限的，它取决于一块
slab分割成若干缓冲区（对象）以及所需的其他空间以后的剩余，以及高速缓存中每个缓冲行的大小。所以，对每个slab队列都要计算出它的颜色数量，这个数量就保存在colour中，而下一个slab将要使用的颜色则保存在colour_next中。当colour_next达到最大值colour时，就又从0开始，若此周而复始。着色区的大小可以根据（colour_off*colour）算得。

分配了一个kmem_cache_t结构以后，kmem_cache_create()就进行一系列的计算，以确定最佳的slab构成。包括，每个slab由几个页面组成，划分成多少个缓冲区（即“对象”);slab的控制结构kmem_slab_t应该在slab外面集中存放还是就放在每个slab的尾部；每个缓冲区的连接指针应该在slab外面集中放还在slab上与相应缓冲区紧挨着放在一起；还有“颜色”的数量等等。并根据调用参数和计算的结果设置队列头kmem_cache_t结构中的各个参数，包括两个指针ctor和dtor。

最后，将队头kmem_cache_t结构链入cache_cache的next队列中。函数kmem_cache_create（）只是建立了所需的专用缓冲区队列的基础设施，所形成的slab队列是个空队列。而具体slab的创建则需要需要分配缓冲区时，缺发现队列中并无空闲的缓冲区可供分配时，在通过kmem_cache_grow()进行。


###  缓冲区的分配与释放

在建立了一种缓冲区的专用队列以后，就可以通过kmem_cache_alloc()来分配缓冲区了。就上面建立的skbuff_head_cache()队列来说，文件net/core/skbuff.c中是这样进行分配的：

```C++
==================== net/core/skbuff.c 165 166 ====================
165  struct sk_buff *alloc_skb(unsigned int size,int gfp_mask)
166  {
              ......
==================== net/core/skbuff.c 181 186 ====================
181           skb = skb_head_from_pool();
182           if (skb == NULL) {
183                 skb = kmem_cache_alloc(skbuff_head_cache, gfp_mask);
184                 if (skb == NULL)
185                      goto nohead;
186            }
              ......
==================== net/core/skbuff.c 215 215 ====================
215  }

```

函数alloc_skb（）是具体设备驱动程序对kmem_cache_alloc()的包装，在此基础上建立自己的缓冲区管理机制，包括一个sk_buff数据结构的缓冲池，以加快分配数据结构的速度，并防止因具体驱动程序分配/释放缓冲区不当而引起问题。这样，就把具体的设备驱动程序与kmem_cache_alloc()分隔开了。

要分配一个sk_buff数据结构，先通过skb_head_from_pool()试试缓冲池。如果在缓冲池中得不到，那就要进一步通过kmem_cache_alloc（）分配，这就是我们关心的。其代码在mm/slab.c中：

```C++
==================== mm/slab.c 1506 1509 ====================
[alloc_skb()->kmem_cache_alloc()]
1506 void * kmem_cache_alloc (kmem_cache_t *cachep, int flags)
1507 {
1508       return __kmem_cache_alloc(cachep, flags);
1509 }
==================== mm/slab.c 1291 1299 ====================
[alloc_skb()>kmem_cache_alloc()>__kmem_cache_alloc()]
1291 static inline void * __kmem_cache_alloc (kmem_cache_t *cachep, int flags)
1292 {
1293         unsigned long save_flags;
1294         void* objp;
1295
1296         kmem_cache_alloc_head(cachep, flags);
1297         try_again:
1298              local_irq_save(save_flags);
1299 #ifdef CONFIG_SMP
    ......
==================== mm/slab.c 1319 1325 ====================
1319 #else
1320 objp = kmem_cache_alloc_one(cachep);
1321 #endif
1322         local_irq_restore(save_flags);
1323         return objp;
1324         alloc_new_slab:
1325 #ifdef CONFIG_SMP
    ......
==================== mm/slab.c 1328 1336 ====================
1328 #endif
1329         local_irq_restore(save_flags);
1330         if (kmem_cache_grow(cachep, flags))
1331 /* Someone may have stolen our objs.  Doesn't matter, we'll
1332  * just come back here again.
1333  */
1334         goto try_again;
1335         return NULL;
1336 }
```

首先，alloc_skb()中的指针skbuff_head_cache是个全局量，指向相应的slab队列的队列头，因而这里的参数cachep也指向这个队列。


程序中的kmem_cache_alloc_head（）是为调试而设计的，在实际运行的系统中是空函数。我们在这里也不关心多处理器SMP结构，所以这里关键性操作就是kmem_cache_alloc_one（），这是一个宏操作，其定义为：

```C++
1246 /*
1247  * Returns a ptr to an obj in the given cache.
1248  * caller must guarantee synchronization
1249  * #define for the goto optimization 8-)
1250  */
1251 #define kmem_cache_alloc_one(cachep) \
1252 ({ \
1253        slab_t  *slabp; \
1254 \
1255 /* Get slab alloc is to come from. */ \
1256 { \
1257        struct list_head* p = cachep->firstnotfull; \
1258        if (p == &cachep->slabs) \
1259              goto alloc_new_slab; \
1260              slabp = list_entry(p,slab_t, list);  \
1261 } \
1262        kmem_cache_alloc_one_tail(cachep, slabp); \
1263 })
```

上面kmem_cache_alloc()的代码一定要和这个宏定义结合才能明白。从定义中可以看到，第一步是通过slab队列头中的指针firstnotfull,找到该队列中第一个含有空闲队列的slab。如果这个指针指向slab队列的链表（是链中的第一个slab），那就表示队列中已经没有含有空闲对象的slab，所以就转到kmem_cache_alloc（）中的标号alloc_new_slab处，进一步扩充该slab队列。如果找到了含有空闲对象的slab，就调用kmem_cache_alloc_tail（）分配一个空闲对象并返回其指针：

```C++
[alloc_skb()>kmem_cache_alloc()>__kmem_cache_alloc()>kmem_cache_alloc_one_tail()]
1211 static inline void * kmem_cache_alloc_one_tail (kmem_cache_t *cachep,
1212  slab_t *slabp)
1213 {
1214      void *objp;
1215
1216      STATS_INC_ALLOCED(cachep);
1217      STATS_INC_ACTIVE(cachep);
1218      STATS_SET_HIGH(cachep);
1219
1220 /* get obj pointer */
1221      slabp->inuse++;
1222      objp = slabp->s_mem + slabp->free*cachep->objsize;
1223      slabp->free=slab_bufctl(slabp)[slabp->free];
1224
1225      if (slabp->free == BUFCTL_END)
1226 /* slab now full: move to next slab for next alloc */
1227             cachep->firstnotfull = slabp->list.next;
1228 #if DEBUG
    ......
==================== mm/slab.c 1242 1244 ==================
1242 #endif
1243      return objp;
1244 }
```

如前所述，数据结构slab_t中的free记录着下一次可以分配的空闲对象的序号，而s_mem则指向slab中的对象区，所以根据这些数据和本专用队列的对象大小，就可以计算出该空闲对象的起始地址。然后，就通过宏操作slab_bufctl（）改变字段free的值，使它指明下一个空闲对象的序号。

```c++
==================== mm/slab.c 154 155 ====================
154  #define slab_bufctl(slabp) \
155             ((kmem_bufctl_t *)(((slab_t*)slabp)+1))
```

这个宏操作返回一个kmem_bufctl_t数组的地址，这个数组就在slab中数据结构slab_t的上方，紧挨着数据结构slab_t。该数组以当前对象的序号为下标，而数组元素的值则表明下一个空闲对象的序号。改变了slab_t中free字段的值，就隐含着当前对象已被分配。

如果达到了slab的末尾BUFCTL_END,就要调整该slab队列的指针firstnotfull,使它指向队列中的下一个slab。不过，我们假定slab队列中已经不存在含有空闲对象的slab，所以要转到前面代码中的标号alloc_new_slab处，通过kmem_cache_grow来分配一块新的slab，使缓冲区的队列“生长”起来。函数kmem_cache_grow的代码也在mm/slab.c中：

```c++
==================== mm/slab.c 1066 1168 ====================
[alloc_skb()>kmem_cache_alloc()>__kmem_cache_alloc()>kmem_cache_grow()]
1066 /*
1067  * Grow (by 1) the number of slabs within a cache.  This is called by
1068  * kmem_cache_alloc() when there are no active objs left in a cache.
1069  */
1070 static int kmem_cache_grow (kmem_cache_t * cachep, int flags)
1071 {
1072     slab_t  *slabp;
1073     struct page  *page;
1074     void *objp;
1075     size_t  offset;
1076     unsigned int   i, local_flags;
1077     unsigned long  ctor_flags;
1078     unsigned long  save_flags;
1079
1080 /* Be lazy and only check for valid flags here,
1081  * keeping it out of the critical path in kmem_cache_alloc().
1082  */
1083     if (flags & ~(SLAB_DMA|SLAB_LEVEL_MASK|SLAB_NO_GROW))
1084        BUG();
1085     if (flags & SLAB_NO_GROW)
1086        return 0;
1087
1088 /*
1089  * The test for missing atomic flag is performed here, rather than
1090  * the more obvious place, simply to reduce the critical path length
1091  * in kmem_cache_alloc(). If a caller is seriously mis-behaving they
1092  * will eventually be caught here (where it matters).
1093  */
1094     if (in_interrupt() && (flags & SLAB_LEVEL_MASK) != SLAB_ATOMIC)
1095        BUG();
1096
1097     ctor_flags = SLAB_CTOR_CONSTRUCTOR;
1098     local_flags = (flags & SLAB_LEVEL_MASK);
1099     if (local_flags == SLAB_ATOMIC)
1100 /*
1101  * Not allowed to sleep.  Need to tell a constructor about
1102  * this - it might need to know...
1103  */
1104        ctor_flags |= SLAB_CTOR_ATOMIC;
1105
1106 /* About to mess with non-constant members - lock. */
1107    spin_lock_irqsave(&cachep->spinlock, save_flags);
1108
1109 /* Get colour for the slab, and cal the next value. */
1110     offset = cachep->colour_next;
1111     cachep->colour_next++;
1112     if (cachep->colour_next >= cachep->colour)
1113     cachep->colour_next = 0;
1114     offset *= cachep->colour_off;
1115     cachep->dflags |= DFLGS_GROWN;
1116
1117     cachep->growing++;
1118     spin_unlock_irqrestore(&cachep->spinlock, save_flags);
1119
1120 /* A series of memory allocations for a new slab.
1121  * Neither the cache-chain semaphore, or cache-lock, are
1122  * held, but the incrementing c_growing prevents this
1123  * cache from being reaped or shrunk.
1124  * Note: The cache could be selected in for reaping in
1125  * kmem_cache_reap(), but when the final test is made the
1126  * growing value will be seen.
1127  */
1128
1129 /* Get mem for the objs. */
1130     if (!(objp = kmem_getpages(cachep, flags)))
1131        goto failed;
1132
1133 /* Get slab management. */
1134     if (!(slabp = kmem_cache_slabmgmt(cachep, objp, offset, local_flags)))
1135        goto opps1;
1136
1137 /* Nasty!!!!!! I hope this is OK. */
1138     i = 1 << cachep->gfporder;
1139     page = virt_to_page(objp);
1140     do {
1141        SET_PAGE_CACHE(page, cachep);
1142        SET_PAGE_SLAB(page, slabp);
1143        PageSetSlab(page);
1144        page++;
1145     } while (--i);
1146
1147     kmem_cache_init_objs(cachep, slabp, ctor_flags);
1148
1149     spin_lock_irqsave(&cachep->spinlock, save_flags);
1150     cachep->growing--;
1151
1152 /* Make slab active. */
1153     list_add_tail(&slabp->list,&cachep->slabs);
1154     if (cachep->firstnotfull == &cachep->slabs)
1155        cachep->firstnotfull = &slabp->list;
1156     STATS_INC_GROWN(cachep);
1157     cachep->failures = 0;
1158
1159     spin_unlock_irqrestore(&cachep->spinlock, save_flags);
1160     return 1;
1161 opps1:
1162     kmem_freepages(cachep, objp);
1163 failed:
1164     spin_lock_irqsave(&cachep->spinlock, save_flags);
1165     cachep->growing--;
1166     spin_unlock_irqrestore(&cachep->spinlock, save_flags);
1167     return 0;
1168 }
```

函数kmem_cache_grow根据队列头中的参数gfporder分配若干连续的物理内存页面，并将这些页面构造slab,链入给定的slab队列。对参数进行一些检查以后，就计算出下一块slab应有的着色区大小。然后，通过kmem_getpages（）分配用于具体对象缓冲区的页面，这个函数最终调用alloc_pages（）分配空闲页面。分配了用于对象本身的内存页面后，还要通过kmem_cache_slabmgmt（）建立起slab的管理信息。其代码在mm/slab.c中。

```C++
==================== mm/slab.c 996 1021 ====================
[alloc_skb()>kmem_cache_alloc()>__kmem_cache_alloc()>kmem_cache_grow()>kmem_cache_slabmgmt()]
996  /* Get the memory for a slab management obj. */
997  static inline slab_t * kmem_cache_slabmgmt (kmem_cache_t *cachep,
998 void *objp, int colour_off, int local_flags)
999  {
1000     slab_t *slabp;
1001
1002     if (OFF_SLAB(cachep)) {
1003 /* Slab management obj is off-slab. */
1004        slabp = kmem_cache_alloc(cachep->slabp_cache, local_flags);
1005        if (!slabp)
1006           return NULL;
1007     } else {
1008 /* FIXME: change to
1009        slabp = objp
1010  * if you enable OPTIMIZE
1011  */
1012        slabp = objp+colour_off;
1013        colour_off += L1_CACHE_ALIGN(cachep->num *
1014        sizeof(kmem_bufctl_t) + sizeof(slab_t));
1015     }
1016     slabp->inuse = 0;
1017     slabp->colouroff = colour_off;
1018     slabp->s_mem = objp+colour_off;
1019
1020     return slabp;
1021 }
```

如前所述，小对象的slab控制结构slab_t与对象本身共存于同一slab上，而大对象的控制结构则游离域slab之外。但是，大对象的控制结构也是slab_t,存在于这种数据结构专设的slab上，也有其专用的slab队列。所以，如果是大对象就通过kmem_cache_alloc()分配一个slab_t，否则就用小对象slab低端的一部分空间用于其控制对象，不过在此之前要空出一小块着色区。

对分配用于slab的每个页面的page数据结构，要通过宏操作SET_PAGE_CACHE和SET_PAGE_SLAB，设置其链接指针prev和next，使他们分别指向所属的slab和slab队列。同时，还要把page结构中的PG_slab标志位设成1，以表明页面的用途。


最后，通过kmem_cache_init_objs()进行slab的初始化：

```C++
==================== mm/slab.c 1023 1030 ====================
[alloc_skb()>kmem_cache_alloc()>__kmem_cache_alloc()>kmem_cache_grow()>kmem_cache_init_objs()]
1023 static inline void kmem_cache_init_objs (kmem_cache_t * cachep,
1024 slab_t * slabp, unsigned long ctor_flags)
1025 {
1026     int i;
1027
1028     for (i = 0; i < cachep->num; i++) {
1029     void* objp = slabp->s_mem+cachep->objsize*i;
1030 #if DEBUG
    ......
==================== mm/slab.c 1037 1046 ====================
1037 #endif
1038
1039 /*
1040  * Constructors are not allowed to allocate memory from
1041  * the same cache which they are a constructor for
1042  * Otherwise, deadlock. They must also be threaded.
1043  */
1044     if (cachep->ctor)
1045        cachep->ctor(objp, cachep, ctor_flags);
1046 #if DEBUG
    ......
==================== mm/slab.c 1059 1064 ====================
1059 #endif
1060     slab_bufctl(slabp)[i] = i+1;
1061 }
1062     slab_bufctl(slabp)[i-1] = BUFCTL_END;
1063     slabp->free = 0;
1064 }
```

这里的初始化包括了对具体对象构造函数的调用。对于sk_buff数据结构，这个函数就是skb_headerinit（）。缓冲区队列“成长”了一些之后，就一定要有空闲缓冲区可供分配了。所以转回标号try_again处再试一遍。

这样，就构成了一个多层次的缓冲区分配机制。位于最高层的是缓冲区的分配，在我们这个情景中就是alloc_skb(),具体则是先通过skb_head_from_pool(),从缓冲池，即已经分配的slab块中分配。如果失败的话，就往下跑一层从slab队列中通过kmem_cache_alloc()分配。要是slab队列中已经没有空闲区的slab，那就往下跑一层，通过kmem_cache_grow(),分配若干页面而构造出一个slab块。


那么，缓冲区队列是否单调地成长而不缩小呢？我们在以前提到过，kswapd定时地调用调用kmem_cache_reap()来“收割”。也就是说，依次检查若干专用缓冲区slab队列，看看是否有完全空闲slab存在。有的话就将这些slab占用的内存页面释放。

再来看专用缓冲区的释放，这里由kmem_cache_free完成。其代码在mm/slab.c中：

```C++
==================== mm/slab.c 1554 1557 ====================
1554 void kmem_cache_free (kmem_cache_t *cachep, void *objp)
1555 {
1556     unsigned long flags;
1557 #if DEBUG
    ......
==================== mm/slab.c 1561 1566 ====================
1561 #endif
1562
1563     local_irq_save(flags);
1564     __kmem_cache_free(cachep, objp);
1565     local_irq_restore(flags);
1566 }
```

显然，操作的主体是kmem_cache_free(),这里只是在操作期间把中断暂时关闭。

```C++
[kmem_cache_free()>__kmem_cache_free()]
1466 /*
1467  * __kmem_cache_free
1468  * called with disabled ints
1469  */
1470 static inline void __kmem_cache_free (kmem_cache_t *cachep, void* objp)
1471 {
1472 #ifdef CONFIG_SMP
    ......
==================== mm/slab.c 1493 1496 ====================
1493 #else
1494     kmem_cache_free_one(cachep, objp);
1495 #endif
1496 }
```

我们这里不关心多处理器SMP结构，而函数kmem_cache_free_one（）的代码也在同一个文件中：

```C++
=================== mm/slab.c 1367 1380 ====================
[kmem_cache_free()>__kmem_cache_free()>kmem_cache_free_one()]
1367 static inline void kmem_cache_free_one(kmem_cache_t *cachep, void *objp)
1368 {
1369     slab_t* slabp;
1370
1371     CHECK_PAGE(virt_to_page(objp));
1372 /* reduces memory footprint
1373  *
1374     if (OPTIMIZE(cachep))
1375        slabp = (void*)((unsigned long)objp&(~(PAGE_SIZE-1)));
1376  else
1377  */
1378     slabp = GET_PAGE_SLAB(virt_to_page(objp));
1379
1380 #if DEBUG
    ......
==================== mm/slab.c 1402 1448 ====================
1402 #endif
1403 {
1404     unsigned int objnr = (objp-slabp->s_mem)/cachep->objsize;
1405
1406     slab_bufctl(slabp)[objnr] = slabp->free;
1407     slabp->free = objnr;
1408 }
1409     STATS_DEC_ACTIVE(cachep);
1410
1411 /* fixup slab chain */
1412     if (slabp->inuse-- == cachep->num)
1413        goto moveslab_partial;
1414     if (!slabp->inuse)
1415 goto moveslab_free;
1416 return;
1417
1418 moveslab_partial:
1419       /* was full.
1420  * Even if the page is now empty, we can set c_firstnotfull to
1421  * slabp: there are no partial slabs in this case
1422  */
1423 {
1424     struct list_head *t = cachep->firstnotfull;
1425
1426     cachep->firstnotfull = &slabp->list;
1427     if (slabp->list.next == t)
1428        return;
1429     list_del(&slabp->list);
1430     list_add_tail(&slabp->list, t);
1431     return;
1432 }
1433 moveslab_free:
1434 /*
1435  * was partial, now empty.
1436  * c_firstnotfull might point to slabp
1437  * FIXME: optimize
1438  */
1439 {
1440     struct list_head *t = cachep->firstnotfull->prev;
1441
1442     list_del(&slabp->list);
1443     list_add_tail(&slabp->list, &cachep->slabs);
1444     if (cachep->firstnotfull == &slabp->list)
1445        cachep->firstnotfull = t->next;
1446     return;
1447 }
1448 }
```





