## Memory Management

在i386CPU将一个线性地址映射成物理地址的过程中。如果该地址的映射已经建立，但还发现相应页面表项或目录项的P（Present）标记位为0，则表示相应的物理页面不在内存，从而无法完成本次内存访问。从理论上说，也许应该把这种情况称为“受阻”而不是“失败”，因为映射的关系毕竟已经建立，理应与尚未建立映射的情况有所区别，所以我们称之为“断开”。但是，CPU的MMU硬件并不区分这两种的情况，只要P标记位0就都认为是页面映射失败，CPU就会产生一次“页面异常”（Page Fault）。事实上，CPU在映射过程中首先看的就是页面表项或目录项的P标记位。只要P标记位为0，其余各个位段的值就无意义了。至于当一个页面不在内存中时，利用页面表项指向一个盘上页面，那是软件的事。所以，区分失败的原因到底是因为页面不在内存，还是因为映射尚未建立，乃是软件，也就是页面异常处理程序的事。在“越界访问”的情景中，我们曾看到在函数handle_pte_fault()中的开头几行：

```c++
==================== mm/memory.c 1153 1175 ====================
[do_page_fault()>handle_mm_fault()>handle_pte_fault()]
1153 static inline int handle_pte_fault(struct mm_struct *mm,
1154 struct vm_area_struct * vma, unsigned long address,
1155 int write_access, pte_t * pte)
1156 {
1157 pte_t entry;
1158
1159 /*
1160  * We need the page table lock to synchronize with kswapd
1161  * and the SMP-safe atomic PTE updates.
1162  */
1163    spin_lock(&mm->page_table_lock);
1164    entry = *pte;
1165    if (!pte_present(entry)) {
1166 /*
1167  * If it truly wasn't present, we know that kswapd
1168  * and the PTE updates will not touch it later. So
1169  * drop the lock.
1170  */
1171        spin_unlock(&mm->page_table_lock);
1172        if (pte_none(entry))
1173            return do_no_page(mm, vma, address, write_access, pte);
1174        return do_swap_page(mm, vma, address, pte, pte_to_swp_entry(entry), write_access);
1175 }
```

这里，首先区分的是pte_present(),也就是检查表项中的P标记位，看看物理页面是否在内存中。如果不在，则进而通过pte_none()检查表项是否为空，即全0。如果为空就说明映射尚未建立，所以要调用do_no_page()。这在以前的情景中已经看到过了。反之，如果非空，就说明映射已经建立，只是物理页面不在内存中，所以要通过do_swap_page(),从交换设备上换入这个页面。本情景在handle_pte_fault之前的处理以及执行旅行都与越界访问的情景相同，所以我们直接进入do_swap_page():

```c++
1018 static int do_swap_page(struct mm_struct * mm,
1019 struct vm_area_struct * vma, unsigned long address,
1020 pte_t * page_table, swp_entry_t entry, int write_access)
1021 {
1022    struct page *page = lookup_swap_cache(entry);
1023    pte_t pte;
1024
1025    if (!page) {
1026        lock_kernel();
1027        swapin_readahead(entry);
1028        page = read_swap_cache(entry);
1029        unlock_kernel();
1030        if (!page)
1031            return -1;
1032
1033        flush_page_to_ram(page);
1034        flush_icache_page(vma, page);
1035    }
1036
1037    mm->rss++;
1038
1039    pte = mk_pte(page, vma->vm_page_prot);
1040
1041 /*
1042  * Freeze the "shared"ness of the page, ie page_count + swap_count.
1043  * Must lock page before transferring our swap count to already
1044  * obtained page count.
1045  */
1046    lock_page(page);
1047    swap_free(entry);
1048    if (write_access && !is_page_shared(page))
1049    pte = pte_mkwrite(pte_mkdirty(pte));
1050    UnlockPage(page);
1051
1052    set_pte(page_table, pte);
1053 /* No need to invalidate - it was non-present before */
1054    update_mmu_cache(vma, address, pte);
1055    return 1; /* Minor fault */
1056 }
```

